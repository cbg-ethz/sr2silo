{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"sr2silo","text":""},{"location":"#wrangele-bam-nucleotide-alignments-to-cleartext-alignments","title":"Wrangele BAM nucleotide alignments to cleartext alignments","text":""},{"location":"#general-use-convert-nucleotide-alignment-reads-cigar-in-bam-to-cleartext-json","title":"General Use: Convert Nucleotide Alignment Reads - CIGAR in .BAM to Cleartext JSON","text":"<p>sr2silo can convert millions of Short-Read nucleotide reads in the form of <code>.bam</code> CIGAR alignments to cleartext alignments compatible with LAPIS-SILO v0.8.0+. It gracefully extracts insertions and deletions. Optionally, sr2silo can translate and align each read using diamond / blastX, handling insertions and deletions in amino acid sequences as well.</p> <p>Your input <code>.bam/.sam</code> with one line as: <pre><code>294 163 NC_045512.2 79  60  31S220M =   197 400 CTCTTGTAGAT FGGGHHHHLMM ...\n</code></pre></p> <p>sr2silo outputs per read a JSON (compatible with LAPIS-SILO v0.8.0+):</p> <pre><code>{\n  \"readId\": \"AV233803:AV044:2411515907:1:10805:5199:3294\",\n  \"sampleId\": \"A1_05_2024_10_08\",\n  \"batchId\": \"20241024_2411515907\",\n  \"samplingDate\": \"2024-10-08\",\n  \"locationName\": \"Lugano (TI)\",\n  \"locationCode\": \"5\",\n  \"sr2siloVersion\": \"1.3.0\",\n  \"main\": {\n    \"sequence\": \"CGGTTTCGTCCGTGTTGCAGCCG...GTGTCAACATCTTAAAGATGGCACTTGTG\",\n    \"insertions\": [\"10:ACTG\", \"456:TACG\"],\n    \"offset\": 4545\n  },\n  \"S\": {\n    \"sequence\": \"MESLVPGFNEKTHVQLSLPVLQVRVRGFGDSVEEVLSEARQHLKDGTCGLVEVEKGV\",\n    \"insertions\": [\"23:A\", \"145:KLM\"],\n    \"offset\": 78\n  },\n  \"ORF1a\": {\n    \"sequence\": \"XXXMESLVPGFNEKTHVQLSLPVLQVRVRGFGDSVEEVLSEARQHLKDGTCGLV\",\n    \"insertions\": [\"2323:TG\", \"2389:CA\"],\n    \"offset\": 678\n  },\n  \"E\": null,\n  \"M\": null,\n  \"N\": null,\n  \"ORF1b\": null,\n  \"ORF3a\": null,\n  \"ORF6\": null,\n  \"ORF7a\": null,\n  \"ORF7b\": null,\n  \"ORF8\": null,\n  \"ORF10\": null\n}\n</code></pre> <p>The total output is handled in an <code>.ndjson.zst</code>.</p>"},{"location":"#resource-requirements","title":"Resource Requirements","text":"<p>When running sr2silo, particularly the <code>process-from-vpipe</code> command, be aware of memory and storage requirements:</p> <ul> <li>Standard configuration uses 8GB RAM and one CPU core</li> <li>Processing batches of 100k reads requires ~3GB RAM plus ~3GB for Diamond</li> <li>Temporary storage needs (especially on clusters) can reach 30-50GB</li> </ul> <p>For detailed information about resource requirements, especially for cluster environments, please refer to the Resource Requirements documentation.</p>"},{"location":"#wrangling-short-read-genomic-alignments-for-silo-database","title":"Wrangling Short-Read Genomic Alignments for SILO Database","text":"<p>Originally this was started for wrangling short-read genomic alignments from wastewater-sampling, into a format for easy import into Loculus and its sequence database SILO.</p> <p>sr2silo is designed to process nucleotide alignments from <code>.bam</code> files with metadata, translate and align reads in amino acids, gracefully handling all insertions and deletions and upload the results to the backend LAPIS-SILO v0.8.0+.</p> <p>Output Format for LAPIS-SILO v0.8.0+: - Metadata fields use camelCase naming (e.g., <code>readId</code>, <code>sampleId</code>, <code>batchId</code>) to align with Loculus standards - Metadata fields are at the root level (no nested \"metadata\" object) - Genomic segments use a structured format with <code>sequence</code>, <code>insertions</code>, and <code>offset</code> fields - The main nucleotide segment is required and contains the primary alignment - Gene segments (S, ORF1a, etc.) contain amino acid sequences or <code>null</code> if empty - Insertions use the format <code>\"position:sequence\"</code> (e.g., <code>\"123:ACGT\"</code>)</p> <p>Output Schema Configuration:</p> <p>The output schema is defined in <code>src/sr2silo/silo_read_schema.py</code> using Pydantic models with field aliases for camelCase output. To modify the metadata fields:</p> <ol> <li>Edit <code>src/sr2silo/silo_read_schema.py</code> - Add/modify fields in <code>ReadMetadata</code> class</li> <li>Update <code>resources/silo/database_config.yaml</code> - Ensure field names match the Pydantic aliases</li> <li>Run validation: <code>python tests/test_database_config_validation.py</code></li> </ol> <p>The validation ensures your Pydantic schema matches the SILO database configuration.</p> <p>For the V-Pipe to Silo implementation we include the following metadata fields at the root level: <pre><code>{\n  \"readId\": \"AV233803:AV044:2411515907:1:10805:5199:3294\",\n  \"sampleId\": \"A1_05_2024_10_08\",\n  \"batchId\": \"20241024_2411515907\",\n  \"samplingDate\": \"2024-10-08\",\n  \"locationName\": \"Lugano (TI)\",\n  \"locationCode\": \"5\",\n  \"sr2siloVersion\": \"1.3.0\"\n}\n</code></pre></p>"},{"location":"#setting-up-the-repository","title":"Setting up the repository","text":"<p>To build the package and maintain dependencies, we use Poetry. In particular, it's good to install it and become familiar with its basic functionalities by reading the documentation.</p>"},{"location":"#installation","title":"Installation","text":"<p>sr2silo can be installed either from Bioconda or from source.</p>"},{"location":"#install-from-bioconda","title":"Install from Bioconda","text":"<p>The easiest way to install sr2silo is through the Bioconda channel:</p> <pre><code># Add necessary channels if you haven't already\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n\n# Install sr2silo\nconda install sr2silo\n</code></pre>"},{"location":"#install-from-source","title":"Install from Source","text":"<p>For development purposes or to install the latest version, you can install from source using Poetry:</p> <p>The project uses a modular environment system to separate core functionality, development requirements, and workflow dependencies. Environment files are located in the <code>environments/</code> directory:</p>"},{"location":"#core-environment-setup","title":"Core Environment Setup","text":"<p>For basic usage of sr2silo: <pre><code>make setup\n</code></pre> This creates the core conda environment with essential dependencies and installs the package using Poetry.</p>"},{"location":"#development-environment","title":"Development Environment","text":"<p>For development work: <pre><code>make setup-dev\n</code></pre> This command sets up the development environment with Poetry.</p>"},{"location":"#workflow-environment","title":"Workflow Environment","text":"<p>For working with the snakemake workflow: <pre><code>make setup-workflow\n</code></pre> This creates an environment specifically configured for running the sr2silo in snakemake workflows.</p>"},{"location":"#all-environments","title":"All Environments","text":"<p>You can set up all environments at once: <pre><code>make setup-all\n</code></pre></p>"},{"location":"#additional-setup-for-development","title":"Additional Setup for Development","text":"<p>After setting up the development environment: <pre><code>conda activate sr2silo-dev\npoetry install --with dev\npoetry run pre-commit install\n</code></pre></p>"},{"location":"#run-tests","title":"Run Tests","text":"<p><pre><code>make test\n</code></pre> or <pre><code>conda activate sr2silo-dev\npytest\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":"<p>sr2silo follows a two-step workflow:</p> <ol> <li>Process data: <code>sr2silo process-from-vpipe --help</code></li> <li>Submit to Loculus: <code>sr2silo submit-to-loculus --help</code></li> </ol> <pre><code># Example: Process V-Pipe data\nsr2silo process-from-vpipe \\\n    --input-file input.bam \\\n    --sample-id SAMPLE_001 \\\n    --timeline-file timeline.tsv \\\n    --output-fp output.ndjson\n\n# Example: Submit to Loculus (use environment variables for credentials)\nexport KEYCLOAK_TOKEN_URL=https://auth.example.com/token\nexport BACKEND_URL=https://api.example.com/submit\nexport GROUP_ID=123\nexport USERNAME=your-username\nexport PASSWORD=your-password\n\nsr2silo submit-to-loculus --processed-file output.ndjson.zst\n</code></pre> <p>Note: Use environment variables for credentials to avoid exposing sensitive information in command history.</p> <p>Note: The <code>--lapis-url</code> parameter is optional. If not provided, sr2silo uses default SARS-CoV-2 references (NC_045512.2). See <code>sr2silo process-from-vpipe --help</code> for details.</p>"},{"location":"#environment-variable-configuration","title":"Environment Variable Configuration","text":"<p>sr2silo supports flexible configuration through environment variables, making it easy to use in different deployment scenarios including conda packages and pip installations.</p> <p>Note: CLI parameters override environment variables</p> <p>Common configuration via environment variables: <pre><code># Authentication credentials (recommended approach for security)\nexport KEYCLOAK_TOKEN_URL=https://auth.example.com/token\nexport BACKEND_URL=https://backend.example.com/api\nexport GROUP_ID=123\nexport USERNAME=your-username\nexport PASSWORD=your-password\n\n# Run with environment variables set\nsr2silo process-from-vpipe \\\n    --input-file input.bam \\\n    --sample-id SAMPLE_001 \\\n    --timeline-file /path/to/timeline.tsv \\\n    --output-fp output.ndjson\n\n# Submission using environment variables for credentials\nsr2silo submit-to-loculus \\\n    --processed-file output.ndjson.zst\n</code></pre></p>"},{"location":"contributing/","title":"Overview","text":"<p>We encourage contributions.</p>"},{"location":"api/loculus/","title":"sr2silo.silo","text":""},{"location":"api/loculus/#sr2silo.loculus.LoculusClient","title":"<code>sr2silo.loculus.LoculusClient</code>","text":"<p>Client for interacting with the Loculus API.</p>"},{"location":"api/loculus/#sr2silo.loculus.LoculusClient.__init__","title":"<code>__init__(token_url, backend_url, organism)</code>","text":"<p>Initialize the Loculus client.</p> <p>Parameters:</p> Name Type Description Default <code>token_url</code> <code>str</code> <p>URL for authentication token endpoint</p> required <code>backend_url</code> <code>str</code> <p>Base URL for backend endpoint</p> required <code>organism</code> <code>str</code> <p>Organism identifier (e.g., 'sc2', 'sars-cov-2')</p> required"},{"location":"api/loculus/#sr2silo.loculus.LoculusClient.authenticate","title":"<code>authenticate(username, password)</code>","text":"<p>Authenticate with the Loculus API.</p>"},{"location":"api/loculus/#sr2silo.loculus.LoculusClient.request_upload","title":"<code>request_upload(group_id, numberFiles)</code>","text":"<p>Request S3 pre-signed URLs to upload files.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>int</code> <p>The group ID for the upload request</p> required <code>numberFiles</code> <code>int</code> <p>The number of files to request upload URLs for</p> required <p>Returns:</p> Type Description <code>List[RequestUploadResponse]</code> <p>List of request-upload response objects containing fileId and pre-signed URL</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the request fails or authentication token is missing</p>"},{"location":"api/loculus/#sr2silo.loculus.LoculusClient.submit","title":"<code>submit(group_id, metadata_file_path, processed_file_path, nucleotide_alignment, submission_id=None, data_use_terms_type='OPEN', resubmit_duplicate=False)</code>","text":"<p>Submit data to the Lapis API using pre-signed upload approach.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>int</code> <p>The group ID for the submission</p> required <code>metadata_file_path</code> <code>Path</code> <p>Path to the metadata TSV file</p> required <code>processed_file_path</code> <code>Path</code> <p>Path to the processed data file (e.g., .ndjson.zst)</p> required <code>nucleotide_alignment</code> <code>Path</code> <p>Path to nucleotide alignment file (.bam)</p> required <code>submission_id</code> <code>str | None</code> <p>Unique identifier for this submission            (auto-generated if not provided)</p> <code>None</code> <code>data_use_terms_type</code> <code>str</code> <p>Data use terms type (default: \"OPEN\")</p> <code>'OPEN'</code> <p>Returns:</p> Type Description <code>SubmitResponse</code> <p>Submit response object</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If submission fails or authentication token is missing</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission","title":"<code>sr2silo.loculus.Submission</code>","text":"<p>Submission-related utilities. Methods for generating placeholder FASTA files containing \"NNN\" sequences, and S3 links</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission.__init__","title":"<code>__init__(fasta, s3_link)</code>","text":"<p>Initialize the Submission object.</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission.count_reads","title":"<code>count_reads(silo_input)</code>  <code>staticmethod</code>","text":"<p>Counts the number of reads in a silo input .ndjson.zstd or .ndjson file.</p> <p>Assumption: each line in the file corresponds to one read.</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission.create_metadata_file","title":"<code>create_metadata_file(processed_file, count_reads=False)</code>  <code>staticmethod</code>","text":"<p>Create a metadata TSV file with the required submissionId header.</p> <pre><code>The metadata file will be saved in a 'submission' subdirectory of\nits parent directory.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>processed_file</code> <code>Path</code> <p>Path to the processed file.</p> required <code>count_reads</code> <code>bool</code> <p>Whether to include a countReads column (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Path, str]</code> <p>Tuple of (Path to the created metadata file, submission ID)</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission.generate_placeholder_fasta","title":"<code>generate_placeholder_fasta(submission_ids)</code>  <code>staticmethod</code>","text":"<p>Generates a placeholder FASTA file for each submission ID with \"NNN\" as the sequence.</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission.get_submission_ids_from_tsv","title":"<code>get_submission_ids_from_tsv(file_path)</code>  <code>staticmethod</code>","text":"<p>Reads a TSV file and extracts submission IDs by parsing the \"submissionId\" column.</p>"},{"location":"api/loculus/#sr2silo.loculus.Submission.parse_metadata","title":"<code>parse_metadata(silo_input)</code>  <code>staticmethod</code>","text":"<p>Parses the metadata from a silo input .ndjson.zstd or .ndjson returning all metadata fields but readId as a dictionary with keys in snake_case format for internal Python use.</p> <p>The input JSON uses camelCase (matching SILO database schema), but this method returns snake_case keys for Python conventions.</p> Assumptions <ul> <li>the metadata is stored in the root of the object under the keys</li> <li>each read has the same metadata, up to readId</li> </ul>"},{"location":"api/loculus/#sr2silo.loculus.LapisClient","title":"<code>sr2silo.loculus.LapisClient</code>","text":"<p>Client for interacting with the Lapis API.</p>"},{"location":"api/loculus/#sr2silo.loculus.LapisClient.__init__","title":"<code>__init__(lapisUrl)</code>","text":"<p>Initialize the Lapis client.</p> <p>Parameters:</p> Name Type Description Default <code>lapisUrl</code> <p>Base URL for the Lapis API</p> required"},{"location":"api/loculus/#sr2silo.loculus.LapisClient.referenceGenome","title":"<code>referenceGenome()</code>","text":"<p>Fetch reference genome from the Lapis <code>sample/referenceGenome</code> endpoint.</p> <p>Returns:</p> Type Description <code>dict</code> <p>JSON response as a dictionary.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the request fails.</p>"},{"location":"api/loculus/#sr2silo.loculus.LapisClient.referenceGenomeToFasta","title":"<code>referenceGenomeToFasta(reference_json_string, nucleotide_out_fp, amino_acid_out_fp)</code>  <code>staticmethod</code>","text":"<p>Convert a reference JSON from <code>sample/referenceGenome</code> endpoint     to separate nucleotide and amino acid reference FASTA files.</p> <p>Parameters:</p> Name Type Description Default <code>reference_json_string</code> <code>str</code> <p>JSON string containing reference sequences with                     'nucleotideSequences' and 'genes' sections</p> required <code>nucleotide_out_fp</code> <code>Path</code> <p>Path to the output nucleotide FASTA file</p> required <code>amino_acid_out_fp</code> <code>Path</code> <p>Path to the output amino acid FASTA file</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"contributing/branching-strategy/","title":"Branching Strategy","text":"<p>This project follows a simple but effective branching strategy centered around two main branches:</p>"},{"location":"contributing/branching-strategy/#main-branches","title":"Main Branches","text":"<ol> <li>main: The production-ready branch that always contains stable, releasable code.</li> <li>Always deployable</li> <li>Protected from direct pushes</li> <li> <p>Changes come from merged PRs from the <code>dev</code> branch</p> </li> <li> <p>dev: The development branch where features and fixes are integrated.</p> </li> <li>Used for day-to-day development work</li> <li>Integration branch for features and fixes</li> <li>Code here should pass all tests but may not be production-ready</li> <li>Pull requests target this branch</li> </ol>"},{"location":"contributing/branching-strategy/#feature-development","title":"Feature Development","text":"<p>When working on new features or bug fixes, follow these steps:</p> <ol> <li> <p>Create a feature branch from <code>dev</code>:    <pre><code>git checkout dev\ngit pull origin dev\ngit checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes, commit them, and push to your feature branch:    <pre><code>git add .\ngit commit -m \"Descriptive commit message\"\ngit push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a pull request from your feature branch to the <code>dev</code> branch.</p> </li> <li> <p>After review and approval, your changes will be merged into the <code>dev</code> branch.</p> </li> </ol>"},{"location":"contributing/branching-strategy/#releases","title":"Releases","text":"<p>When the <code>dev</code> branch contains enough features for a release or a scheduled release is due:</p> <ol> <li>Create a pull request from <code>dev</code> to <code>main</code>.</li> <li>Review the changes thoroughly.</li> <li>After approval, merge the pull request to deploy to production.</li> <li>Tag the release in the <code>main</code> branch with a version number.</li> </ol>"},{"location":"contributing/branching-strategy/#hotfixes","title":"Hotfixes","text":"<p>For critical production issues:</p> <ol> <li> <p>Create a hotfix branch from <code>main</code>:    <pre><code>git checkout main\ngit pull origin main\ngit checkout -b hotfix/issue-description\n</code></pre></p> </li> <li> <p>Fix the issue, commit, and push:    <pre><code>git add .\ngit commit -m \"Fix critical issue\"\ngit push origin hotfix/issue-description\n</code></pre></p> </li> <li> <p>Create PRs to both <code>main</code> and <code>dev</code> branches to ensure the fix is applied to both branches.</p> </li> </ol>"},{"location":"usage/resource_requirements/","title":"Resource Requirements","text":"<p>When running sr2silo, especially the <code>--import to loculus</code> command, you should be aware of the following resource requirements:</p>"},{"location":"usage/resource_requirements/#memory-requirements","title":"Memory Requirements","text":"<ul> <li>Standard Resources: The default Snakemake configuration uses 8 GB RAM and one CPU core</li> <li>Actual Usage: </li> <li>sr2silo processes in batches of 100k reads at a time, requiring approximately 3 GB of RAM</li> <li>An additional 3 GB of RAM is needed for running Diamond for amino acid translation and alignment</li> </ul>"},{"location":"usage/resource_requirements/#storage-requirements","title":"Storage Requirements","text":"<ul> <li>Temporary Space: </li> <li>sr2silo itself requires some temporary directory space</li> <li>Diamond may require up to 30 GB of temporary storage</li> <li>Ideally, this should be on high I/O storage</li> </ul>"},{"location":"usage/resource_requirements/#cluster-environment-configuration","title":"Cluster Environment Configuration","text":"<p>When running on a personal computer, standard temporary directories are usually sufficient as long as you have free disk space. However, in a cluster environment:</p> <ol> <li>Set the environment variable <code>TMPDIR</code> to a location with at least 50 GB of free space per run:</li> </ol> <pre><code>export TMPDIR=/path/to/temp/directory\n</code></pre> <ol> <li>Make sure this directory has good I/O performance for optimal processing speed</li> </ol> <p>This configuration will help prevent issues with temporary file storage during the processing of large datasets.</p>"}]}