"""Workflow to process historical samples of V-Pipe nucleotide alignment to
    SILO ready ndjson.zst files.
"""

import yaml
import os
import csv
from pathlib import Path
from datetime import datetime


CONFIG = "workflow/config.yaml"
# CONFIG = "workflow/config_euler.yaml"


configfile: CONFIG


# Load configuration
with open(CONFIG, "r") as file:
    config = yaml.safe_load(file)


def get_sample_batch_mapping():
    """Read timeline file and filter by date range and location codes."""

    # Read the timeline file
    sample_batch_mapping = {}

    with open(config["TIMELINE_FILE"], "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Skip header row if it exists
        first_row = next(reader, None)
        if first_row and first_row[0] == "sample":
            # This is a header row, continue with the next rows
            pass
        else:
            # This is data, process it
            if first_row and len(first_row) >= 7:
                sample, batch, reads, proto, location_code, date_str, location = (
                    first_row
                )

                # Convert date string to datetime for comparison
                try:
                    sample_date = datetime.strptime(date_str, "%Y-%m-%d")
                    location_code_int = int(location_code)

                    # Filter by date range
                    start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
                    end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

                    if (
                        start_date <= sample_date <= end_date
                        and location_code_int in config["LOCATIONS"]
                    ):
                        sample_batch_mapping[sample] = batch if batch else ""
                except (ValueError, TypeError):
                    pass  # Skip invalid rows

        # Process remaining rows
        for row in reader:
            if len(row) < 7:  # Skip invalid rows
                continue

            sample, batch, reads, proto, location_code, date_str, location = row

            # Convert date string to datetime for comparison
            try:
                sample_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                continue  # Skip rows with invalid date format

            # Convert location_code to int for comparison
            try:
                location_code_int = int(location_code)
            except ValueError:
                continue  # Skip rows with invalid location code

            # Filter by date range
            start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
            end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

            if not (start_date <= sample_date <= end_date):
                continue

            # Filter by location codes
            if location_code_int not in config["LOCATIONS"]:
                continue

            # Add to mapping (batch can be empty)
            sample_batch_mapping[sample] = batch if batch else ""

    return sample_batch_mapping


def get_batch_id(sample_id):
    """Get the batch id from the filtered sample batch mapping."""
    sample_batch_mapping = get_sample_batch_mapping()
    batch_id = sample_batch_mapping.get(sample_id, "")
    # Handle empty batch_id by returning "nobatch" for file naming
    return batch_id if batch_id else "nobatch"


def get_actual_batch_id(sample_id):
    """Get the actual batch id (can be empty) for processing."""
    sample_batch_mapping = get_sample_batch_mapping()
    return sample_batch_mapping.get(sample_id, "")


def get_input_file_path(sample_id):
    """Get the correct input file path based on whether batch_id exists."""
    batch_id = get_actual_batch_id(sample_id)
    if batch_id:
        # If batch_id exists, use the standard path with batch_id
        return f"{config['BASE_SAMPLE_DIR']}/{sample_id}/{batch_id}/alignments/REF_aln_trim.bam"
    else:
        # If no batch_id, assume the file is directly under the sample directory
        return f"{config['BASE_SAMPLE_DIR']}/{sample_id}/alignments/REF_aln_trim.bam"


# Get the filtered sample-batch mapping
SAMPLE_BATCH_IDS = get_sample_batch_mapping()

# Warning if no samples found
if not SAMPLE_BATCH_IDS:
    print("⚠️  WARNING: No samples found matching the filtering criteria!")
    print(f"   Date range: {config['START_DATE']} to {config['END_DATE']}")
    print(f"   Location codes: {config['LOCATIONS']}")
    print(f"   Timeline file: {config['TIMELINE_FILE']}")
    print("   Please check your configuration and timeline file.")
    print("   The workflow will complete with no jobs to execute.")
else:
    print(f"✓ Found {len(SAMPLE_BATCH_IDS)} samples matching criteria:")
    for sample_id, batch_id in SAMPLE_BATCH_IDS.items():
        print(f"   {sample_id} -> {batch_id}")
    print()


rule all:
    input:
        [
            f"{config['RESULTS_DIR']}/sampleId-{sample_id}_batchId-{get_batch_id(sample_id)}.ndjson.zst"
            for sample_id in SAMPLE_BATCH_IDS.keys()
        ],
        [
            f"{config['RESULTS_DIR']}/uploads/sampleId-{sample_id}_batchId-{get_batch_id(sample_id)}.uploaded"
            for sample_id in SAMPLE_BATCH_IDS.keys()
        ],


rule process_sample:
    """Processes the sample to ndjson, skip upload to loculus.

    This rule demonstrates the flexible parameter approach:
    - Parameters can be provided via CLI arguments (as shown)
    - Or via environment variables (TIMELINE_FILE, PRIMER_FILE, NEXTCLADE_REFERENCE)
    - CLI arguments take precedence over environment variables

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        sample_fp=lambda wildcards: get_input_file_path(wildcards.sample_id),
    output:
        result_fp=f"{config['RESULTS_DIR']}"
        + "/sampleId-{sample_id}_batchId-{batch_id}.ndjson.zst",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        batch_id=lambda wildcards: get_actual_batch_id(wildcards.sample_id),  # Use actual batch_id (can be empty)
        timeline_file=config["TIMELINE_FILE"],
        primers_file=config["PRIMERS_FILE"],
        nuc_reference=config["NUC_REFERENCE"],
        aa_reference=config["NUC_REFERENCE"],
    log:
        "logs/sr2silo/process_sample/sampleId_{sample_id}_batchId_{batch_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        if [ -n "{params.batch_id}" ]; then
            BATCH_ID_ARG="--batch-id {params.batch_id}"
        else
            BATCH_ID_ARG=""
        fi

        sr2silo process-from-vpipe \
            --input-file {input.sample_fp} \
            --sample-id {params.sample_id} \
            $BATCH_ID_ARG \
            --timeline-file {params.timeline_file} \
            --primer-file {params.primers_file} \
            --output-fp {output.result_fp} \
            --reference {params.nuc_reference} > {log} 2>&1
        """


rule submit_to_loculus:
    """Submits the processed sample to loculus.

    This rule now uses CLI parameters with environment variable fallbacks.
    The sr2silo command will automatically use environment variables
    if CLI parameters are not provided.

    Required configuration (via config file or environment variables):
        - KEYCLOAK_TOKEN_URL
        - SUBMISSION_URL
        - GROUP_ID
        - USERNAME
        - PASSWORD

    CLI parameters take precedence over environment variables.

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        result_fp=f"{config['RESULTS_DIR']}"
        + "/sampleId-{sample_id}_batchId-{batch_id}.ndjson.zst",
    output:
        flag=f"{config['RESULTS_DIR']}/uploads/sampleId-{{sample_id}}_batchId-{{batch_id}}.uploaded",
    params:
        keycloak_url=config.get("KEYCLOAK_TOKEN_URL", ""),
        submission_url=config.get("SUBMISSION_URL", ""),
        group_id=config.get("GROUP_ID", ""),
        username=config.get("USERNAME", ""),
        password=config.get("PASSWORD", ""),
    log:
        "logs/sr2silo/submit_to_loculus/sampleId_{sample_id}_batchId_{batch_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo submit-to-loculus \
            --processed-file {input.result_fp} \
            --keycloak-token-url "{params.keycloak_url}" \
            --submission-url "{params.submission_url}" \
            --group-id {params.group_id} \
            --username {params.username} \
            --password {params.password} > {log} 2>&1 && \
        mkdir -p $(dirname {output.flag}) && \
        touch {output.flag}
        """
