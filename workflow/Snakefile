"""Workflow to process historical samples of V-Pipe nucleotide alignment to
    SILO ready ndjson.zst files.
"""

import yaml
import os
import csv
from pathlib import Path
from datetime import datetime

CONFIG = "workflow/config.yaml"


configfile: CONFIG


# Load configuration
with open(CONFIG, "r") as file:
    config = yaml.safe_load(file)


def get_sample_list():
    """Read timeline file and filter by date range and location codes."""

    # Read the timeline file
    sample_list = []

    with open(config["TIMELINE_FILE"], "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Skip header row if it exists
        first_row = next(reader, None)
        if first_row and first_row[0] == "sample":
            # This is a header row, continue with the next rows
            pass
        else:
            # This is data, process it
            if first_row and len(first_row) >= 7:
                sample, batch, reads, proto, location_code, date_str, location = (
                    first_row
                )

                # Convert date string to datetime for comparison
                try:
                    sample_date = datetime.strptime(date_str, "%Y-%m-%d")
                    location_code_int = int(location_code)

                    # Filter by date range
                    start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
                    end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

                    if (
                        start_date <= sample_date <= end_date
                        and location_code_int in config["LOCATIONS"]
                    ):
                        sample_list.append(sample)
                except (ValueError, TypeError):
                    pass  # Skip invalid rows

        # Process remaining rows
        for row in reader:
            if len(row) < 7:  # Skip invalid rows
                continue

            sample, batch, reads, proto, location_code, date_str, location = row

            # Convert date string to datetime for comparison
            try:
                sample_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                continue  # Skip rows with invalid date format

            # Convert location_code to int for comparison
            try:
                location_code_int = int(location_code)
            except ValueError:
                continue  # Skip rows with invalid location code

            # Filter by date range
            start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
            end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

            if not (start_date <= sample_date <= end_date):
                continue

            # Filter by location codes
            if location_code_int not in config["LOCATIONS"]:
                continue

            # Add to sample list (avoid duplicates)
            if sample not in sample_list:
                sample_list.append(sample)

    return sample_list


def get_input_file_path(sample_id):
    """Get the input file path for the sample, trying both with and without batch structure."""
    # First try without batch_id structure
    simple_path = f"{config['BASE_SAMPLE_DIR']}/{sample_id}/alignments/REF_aln_trim.bam"
    if Path(simple_path).exists():
        return simple_path

    # If that doesn't exist, we need to find the batch_id by checking the timeline
    with open(config["TIMELINE_FILE"], "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Skip header row if it exists
        first_row = next(reader, None)
        if first_row and first_row[0] == "sample":
            pass  # Header row
        else:
            # Check first row if it's not a header
            if first_row and len(first_row) >= 2 and first_row[0] == sample_id:
                batch_id = first_row[1]
                if batch_id:  # If batch_id exists, try that path
                    batch_path = f"{config['BASE_SAMPLE_DIR']}/{sample_id}/{batch_id}/alignments/REF_aln_trim.bam"
                    if Path(batch_path).exists():
                        return batch_path

        # Process remaining rows
        for row in reader:
            if len(row) >= 2 and row[0] == sample_id:
                batch_id = row[1]
                if batch_id:  # If batch_id exists, try that path
                    batch_path = f"{config['BASE_SAMPLE_DIR']}/{sample_id}/{batch_id}/alignments/REF_aln_trim.bam"
                    if Path(batch_path).exists():
                        return batch_path

    # Default to simple path if nothing else works
    return simple_path


def get_processing_input_path(sample_id):
    """Get the appropriate input path for processing - subsampled if enabled, original otherwise."""
    if config.get("ENABLE_SUBSAMPLING", False):
        return f"{config['RESULTS_DIR']}/subsampled/{sample_id}/REF_aln_trim.bam"
    else:
        return get_input_file_path(sample_id)


# Get the filtered sample list
SAMPLE_IDS = get_sample_list()

# Warning if no samples found
if not SAMPLE_IDS:
    print("⚠️  WARNING: No samples found matching the filtering criteria!")
    print(f"   Date range: {config['START_DATE']} to {config['END_DATE']}")
    print(f"   Location codes: {config['LOCATIONS']}")
    print(f"   Timeline file: {config['TIMELINE_FILE']}")
    print("   Please check your configuration and timeline file.")
    print("   The workflow will complete with no jobs to execute.")
else:
    print(f"✓ Found {len(SAMPLE_IDS)} samples matching criteria:")
    for sample_id in SAMPLE_IDS:
        print(f"   {sample_id}")
    print()


rule all:
    input:
        [
            f"{config['RESULTS_DIR']}/sampleId-{sample_id}.ndjson.zst"
            for sample_id in SAMPLE_IDS
        ],
        [
            f"{config['RESULTS_DIR']}/uploads/sampleId-{sample_id}.uploaded"
            for sample_id in SAMPLE_IDS
        ],
        # Add subsampled BAM files if subsampling is enabled
        [
            f"{config['RESULTS_DIR']}/subsampled/{sample_id}/REF_aln_trim.bam"
            for sample_id in SAMPLE_IDS
        ]
        if config.get("ENABLE_SUBSAMPLING", False)
        else [],
        # Add read count summary if subsampling is enabled
        ["logs/subsampling/read_counts.tsv"]
        if config.get("ENABLE_SUBSAMPLING", False)
        else [],


rule subsample_bam:
    """Subsample BAM files to reduce computational load and standardize input sizes.

    Tracks original and subsampled read counts for downstream analysis.
    Uses samtools view with -s flag for random subsampling.
    """
    input:
        bam=lambda wildcards: get_input_file_path(wildcards.sample_id),
    output:
        bam=f"{config['RESULTS_DIR']}/subsampled/{{sample_id}}/REF_aln_trim.bam",
        log_entry=temp(
            f"{config['RESULTS_DIR']}/subsampled/{{sample_id}}/read_count.tmp"
        ),
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        subsample_fraction=config.get("SUBSAMPLE_FRACTION", 0.1),
        subsample_max_reads=config.get("SUBSAMPLE_MAX_READS", None),
    log:
        "logs/sr2silo/subsample/sampleId_{sample_id}.log",
    conda:
        "envs/subsampling.yaml"
    shell:
        """
        # Create output directory
        mkdir -p $(dirname {output.bam})
        mkdir -p logs/subsampling

        # Count original reads
        ORIGINAL_READS=$(samtools view -c {input.bam})
        echo "Original reads: $ORIGINAL_READS" > {log}

        # Determine subsampling method and fraction
        if [ -n "{params.subsample_max_reads}" ] && [ "{params.subsample_max_reads}" != "None" ]; then
            # Use max reads approach
            MAX_READS={params.subsample_max_reads}
            if [ $ORIGINAL_READS -gt $MAX_READS ]; then
                FRACTION=$(echo "scale=6; $MAX_READS / $ORIGINAL_READS" | bc -l)
                echo "Using max reads: $MAX_READS, calculated fraction: $FRACTION" >> {log}
            else
                FRACTION=1.0
                echo "Original reads ($ORIGINAL_READS) <= max reads ($MAX_READS), keeping all reads" >> {log}
            fi
        else
            # Use fraction approach
            FRACTION={params.subsample_fraction}
            echo "Using fraction: $FRACTION" >> {log}
        fi

        # Subsample the BAM file
        samtools view -s $FRACTION -b {input.bam} > {output.bam} 2>> {log}

        # Count subsampled reads
        SUBSAMPLED_READS=$(samtools view -c {output.bam})
        echo "Subsampled reads: $SUBSAMPLED_READS" >> {log}

        # Create log entry for aggregation
        DATE=$(date +%Y-%m-%d)
        WEEK=$(date +%Y-W%U)
        echo -e "{params.sample_id}\t$DATE\t$ORIGINAL_READS\t$SUBSAMPLED_READS\t$FRACTION\t$WEEK" > {output.log_entry}

        echo "Subsampling completed successfully" >> {log}
        """


rule aggregate_read_counts:
    """Aggregate read count statistics from all subsampled files into a single log file."""
    input:
        log_entries=(
            [
                f"{config['RESULTS_DIR']}/subsampled/{sample_id}/read_count.tmp"
                for sample_id in SAMPLE_IDS
            ]
            if config.get("ENABLE_SUBSAMPLING", False)
            else []
        ),
    output:
        summary="logs/subsampling/read_counts.tsv",
    shell:
        """
        mkdir -p logs/subsampling

        # Create header
        echo -e "sample_id\tdate\toriginal_reads\tsubsampled_reads\tsubsample_fraction\tweek_of_year" > {output.summary}

        # Aggregate all log entries
        if [ "{input.log_entries}" != "" ]; then
            cat {input.log_entries} >> {output.summary}
        fi

        echo "Read count summary created: {output.summary}"
        """


rule process_sample:
    """Processes the sample to ndjson, skip upload to loculus.

    This rule demonstrates the flexible parameter approach:
    - Parameters can be provided via CLI arguments (as shown)
    - Or via environment variables (TIMELINE_FILE, PRIMER_FILE, NEXTCLADE_REFERENCE)
    - CLI arguments take precedence over environment variables

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        sample_fp=lambda wildcards: get_processing_input_path(wildcards.sample_id),
    output:
        result_fp=f"{config['RESULTS_DIR']}" + "/sampleId-{sample_id}.ndjson.zst",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        timeline_file=config["TIMELINE_FILE"],
        lapis_url=config.get("LAPIS_URL", ""),
    log:
        "logs/sr2silo/process_sample/sampleId_{sample_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo process-from-vpipe \
            --input-file {input.sample_fp} \
            --sample-id {params.sample_id} \
            --timeline-file {params.timeline_file} \
            --output-fp {output.result_fp} \
            --lapis-url {params.lapis_url} > {log} 2>&1
        """


rule submit_to_loculus:
    """Submits the processed sample to loculus.

    This rule now uses CLI parameters with environment variable fallbacks.
    The sr2silo command will automatically use environment variables
    if CLI parameters are not provided.

    Required configuration (via config file or environment variables):
        - KEYCLOAK_TOKEN_URL
        - SUBMISSION_URL
        - GROUP_ID
        - USERNAME
        - PASSWORD

    CLI parameters take precedence over environment variables.

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        result_fp=f"{config['RESULTS_DIR']}" + "/sampleId-{sample_id}.ndjson.zst",
    output:
        flag=f"{config['RESULTS_DIR']}/uploads/sampleId-{{sample_id}}.uploaded",
    params:
        keycloak_url=config.get("KEYCLOAK_TOKEN_URL", ""),
        submission_url=config.get("SUBMISSION_URL", ""),
        group_id=config.get("GROUP_ID", ""),
        username=config.get("USERNAME", ""),
        password=config.get("PASSWORD", ""),
    log:
        "logs/sr2silo/submit_to_loculus/sampleId_{sample_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo submit-to-loculus \
            --processed-file {input.result_fp} \
            --keycloak-token-url "{params.keycloak_url}" \
            --submission-url "{params.submission_url}" \
            --group-id {params.group_id} \
            --username {params.username} \
            --password {params.password} > {log} 2>&1 && \
        mkdir -p $(dirname {output.flag}) && \
        touch {output.flag}
        """
