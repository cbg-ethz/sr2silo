"""Workflow to process historical samples of V-Pipe nucleotide alignment to
    SILO ready ndjson.zst files.
"""

import yaml
import os
import sys
import csv
from pathlib import Path
from datetime import datetime

# Prefer Snakemake's notion of the workflow base directory, but fall back to this file's directory for direct execution.
try:
    WORKFLOW_DIR = Path(workflow.basedir)
except NameError:
    WORKFLOW_DIR = Path(__file__).resolve().parent

# Configuration and path resolution notes:
# - When running with --configfile, that config is used (production deployments)
# - When running tests, the suite copies a workflow/config.yaml into a temporary workdir
# - For file paths provided in config (e.g., TIMELINE_FILE, BASE_SAMPLE_DIR), we resolve
#   relative paths first against the runtime workdir, then the config directory, and finally
#   this repository's workflow directory. This keeps CLI runs and CI tests consistent.

# Also capture Snakemake's runtime workdir (when --directory is used), fall back to current CWD
try:
    RUNTIME_DIR = Path(workflow.workdir)
except Exception:
    RUNTIME_DIR = Path.cwd()

# For CI: fallback to workflow/config.yaml in runtime dir
# For production: always use --configfile (no default to config_prod.yaml)
RUNTIME_CONFIG = RUNTIME_DIR / "workflow" / "config.yaml"
CI_CONFIG = WORKFLOW_DIR / "config.yaml"

# Determine fallback config path (used only if --configfile not provided)
if RUNTIME_CONFIG.exists():
    _FALLBACK_CONFIG = RUNTIME_CONFIG
elif CI_CONFIG.exists():
    _FALLBACK_CONFIG = CI_CONFIG
else:
    _FALLBACK_CONFIG = None

# Load fallback config if no --configfile was provided
# (Snakemake's config dict will be empty if no configfile directive and no --configfile)
if _FALLBACK_CONFIG and not config:
    configfile: str(_FALLBACK_CONFIG)

# CONFIG_DIR: directory of the config file for resolving relative paths
# When --configfile is used, we need to find it from the config dict or workflow source
CONFIG_DIR = WORKFLOW_DIR  # default
if config:
    # Try to determine config dir from Snakemake's workflow.source or fall back
    try:
        if hasattr(workflow, 'configfiles') and workflow.configfiles:
            CONFIG_DIR = Path(workflow.configfiles[0]).parent
        elif _FALLBACK_CONFIG:
            CONFIG_DIR = _FALLBACK_CONFIG.parent
    except:
        pass


def resolve_path(p):
    """Resolve a path string relative to (in order): runtime workdir, config dir, workflow dir.

    - Absolute paths are returned unchanged.
    - For relative paths, we return the first existing candidate; if none exist,
      we return the path relative to the config directory (so subsequent creation works).
    """
    p = Path(p)
    if p.is_absolute():
        return p
    for base in (RUNTIME_DIR, CONFIG_DIR, WORKFLOW_DIR):
        candidate = base / p
        if candidate.exists():
            return candidate
    return CONFIG_DIR / p


def resolve_date(date_str):
    """Resolve date string, supporting 'TODAY' and relative dates like '2_WEEKS_AGO', '1_MONTH_AGO'."""
    date_upper = date_str.upper()

    if date_upper == "TODAY":
        return datetime.now().strftime("%Y-%m-%d")

    # Handle relative dates: N_WEEKS_AGO, N_MONTHS_AGO, N_DAYS_AGO
    import re
    from datetime import timedelta
    from dateutil.relativedelta import relativedelta

    # Match patterns like "2_WEEKS_AGO", "1_MONTH_AGO", "30_DAYS_AGO"
    weeks_match = re.match(r"(\d+)_WEEKS?_AGO", date_upper)
    months_match = re.match(r"(\d+)_MONTHS?_AGO", date_upper)
    days_match = re.match(r"(\d+)_DAYS?_AGO", date_upper)

    now = datetime.now()

    if weeks_match:
        weeks = int(weeks_match.group(1))
        target_date = now - timedelta(weeks=weeks)
        return target_date.strftime("%Y-%m-%d")

    elif months_match:
        months = int(months_match.group(1))
        target_date = now - relativedelta(months=months)
        return target_date.strftime("%Y-%m-%d")

    elif days_match:
        days = int(days_match.group(1))
        target_date = now - timedelta(days=days)
        return target_date.strftime("%Y-%m-%d")

    # If no special pattern, return as-is (assume explicit date)
    return date_str


# Resolve dynamic dates in config
config["START_DATE"] = resolve_date(config["START_DATE"])
config["END_DATE"] = resolve_date(config["END_DATE"])


def get_sample_list():
    """Read timeline file and filter by date range and location codes."""

    # Read the timeline file
    sample_list = []

    timeline_fp = resolve_path(config["TIMELINE_FILE"])
    with open(timeline_fp, "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Read header row to detect column names
        header = next(reader, None)
        if not header:
            return sample_list

        # Build column index map - support both COVID and RSV-A column names
        col_map = {name: idx for idx, name in enumerate(header)}
        # Sample column: "sample" (COVID) or "submissionId" (RSV-A)
        sample_col = col_map.get("sample", col_map.get("submissionId", 0))
        batch_col = col_map.get("batch", 1)
        location_code_col = col_map.get("location_code", 5)
        date_col = col_map.get("date", 6)

        # Process all data rows
        for row in reader:
            if len(row) <= max(sample_col, batch_col, location_code_col, date_col):
                continue  # Skip invalid rows

            sample = row[sample_col]
            date_str = row[date_col]
            location_code = row[location_code_col]

            # Convert date string to datetime for comparison
            try:
                sample_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                continue  # Skip rows with invalid date format

            # Convert location_code to int for comparison
            try:
                location_code_int = int(location_code)
            except ValueError:
                continue  # Skip rows with invalid location code

            # Filter by date range
            start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
            end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

            if not (start_date <= sample_date <= end_date):
                continue

            # Filter by location codes
            if location_code_int not in config["LOCATIONS"]:
                continue

            # Add to sample list (avoid duplicates)
            if sample not in sample_list:
                sample_list.append(sample)

    return sample_list


def get_input_file_path(sample_id):
    """Get the input file path for the sample, trying both with and without batch structure."""
    base_dir = resolve_path(config["BASE_SAMPLE_DIR"])

    # First try without batch_id structure
    simple_path = base_dir / sample_id / "alignments" / "REF_aln_trim.bam"
    if simple_path.exists():
        return str(simple_path)

    # If that doesn't exist, we need to find the batch_id by checking the timeline
    timeline_fp = resolve_path(config["TIMELINE_FILE"])
    with open(timeline_fp, "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Read header to get column indices
        header = next(reader, None)
        if not header:
            return str(simple_path)

        col_map = {name: idx for idx, name in enumerate(header)}
        sample_col = col_map.get("sample", col_map.get("submissionId", 0))
        batch_col = col_map.get("batch", 1)

        # Process all rows looking for our sample
        for row in reader:
            if len(row) > max(sample_col, batch_col) and row[sample_col] == sample_id:
                batch_id = row[batch_col]
                if batch_id:  # If batch_id exists, try that path
                    batch_path = (
                        base_dir
                        / sample_id
                        / batch_id
                        / "alignments"
                        / "REF_aln_trim.bam"
                    )
                    if batch_path.exists():
                        return str(batch_path)

    # Default to simple path if nothing else works
    return str(simple_path)


def get_processing_input_path(sample_id):
    """Get the appropriate input path for processing - subsampled if enabled, original otherwise."""
    if config.get("ENABLE_SUBSAMPLING", False):
        return f"{config['RESULTS_DIR']}/subsampled/{sample_id}/REF_aln_trim.bam"
    else:
        return get_input_file_path(sample_id)


# Get the filtered sample list
SAMPLE_IDS = get_sample_list()

# Validate subsampling configuration
if config.get("ENABLE_SUBSAMPLING", False):
    fraction = config.get("SUBSAMPLE_FRACTION")
    max_reads = config.get("SUBSAMPLE_MAX_READS")

    # Check if at least one subsampling parameter is set
    fraction_set = fraction is not None and fraction != "" and fraction != 0
    max_reads_set = max_reads is not None and max_reads != ""

    if not fraction_set and not max_reads_set:
        print(
            "âŒ ERROR: ENABLE_SUBSAMPLING is true but neither SUBSAMPLE_FRACTION nor SUBSAMPLE_MAX_READS is set!"
        )
        print("   Please set at least one of:")
        print("   - SUBSAMPLE_FRACTION (e.g., 0.1 for 10%)")
        print("   - SUBSAMPLE_MAX_READS (e.g., 1000 for max 1000 reads)")
        print("   Or set ENABLE_SUBSAMPLING: false to disable subsampling")
        sys.exit(1)

    # Log the subsampling mode that will be used
    if fraction_set and max_reads_set:
        print(
            f"ðŸŽ¯ Subsampling Mode 3: {fraction*100:.1f}% of reads, capped at {max_reads:,} reads"
        )
    elif max_reads_set:
        print(f"ðŸŽ¯ Subsampling Mode 2: Maximum {max_reads:,} reads")
    elif fraction_set:
        print(f"ðŸŽ¯ Subsampling Mode 1: {fraction*100:.1f}% of reads")

# Warning if no samples found
if not SAMPLE_IDS:
    print("âš ï¸  WARNING: No samples found matching the filtering criteria!")
    print(f"   Date range: {config['START_DATE']} to {config['END_DATE']}")
    print(f"   Location codes: {config['LOCATIONS']}")
    print(f"   Timeline file: {config['TIMELINE_FILE']}")
    print("   Please check your configuration and timeline file.")
    print("   The workflow will complete with no jobs to execute.")
else:
    print(f"âœ“ Found {len(SAMPLE_IDS)} samples matching criteria:")
    for sample_id in SAMPLE_IDS:
        print(f"   {sample_id}")
    print()


rule all:
    input:
        [
            f"{config['RESULTS_DIR']}/sampleId-{sample_id}.ndjson.zst"
            for sample_id in SAMPLE_IDS
        ],
        [
            f"{config['RESULTS_DIR']}/uploads/sampleId-{sample_id}.uploaded"
            for sample_id in SAMPLE_IDS
        ],
        # Add subsampled BAM files if subsampling is enabled
        [
            f"{config['RESULTS_DIR']}/subsampled/{sample_id}/REF_aln_trim.bam"
            for sample_id in SAMPLE_IDS
        ]
        if config.get("ENABLE_SUBSAMPLING", False)
        else [],
        # Add read count summary if subsampling is enabled
        [f"{config['LOG_DIR']}/subsampling/read_counts.tsv"]
        if config.get("ENABLE_SUBSAMPLING", False)
        else [],


rule subsample_bam:
    """Subsample BAM files to reduce computational load and standardize input sizes.

    Tracks original and subsampled read counts for downstream analysis.
    Uses samtools view with -s flag for random subsampling.
    """
    input:
        bam=lambda wildcards: get_input_file_path(wildcards.sample_id),
    output:
        bam=f"{config['RESULTS_DIR']}/subsampled/{{sample_id}}/REF_aln_trim.bam",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        subsample_fraction=config.get("SUBSAMPLE_FRACTION", "None"),
        subsample_max_reads=config.get("SUBSAMPLE_MAX_READS", "None"),
    log:
        f"{config['LOG_DIR']}/sr2silo/subsample/sampleId_{{sample_id}}.log",
    conda:
        "envs/subsampling.yaml"
    shell:
        """
        # Create output directory
        mkdir -p $(dirname {output.bam})
        mkdir -p logs/subsampling

        # Count original reads
        ORIGINAL_READS=$(samtools view -c {input.bam})
        echo "Original reads: $ORIGINAL_READS" > {log}

        # Determine subsampling mode and calculate final fraction
        SUBSAMPLE_FRACTION="{params.subsample_fraction}"
        SUBSAMPLE_MAX_READS="{params.subsample_max_reads}"

        # Check if fraction is set (not None, not empty, not "None")
        if [ "$SUBSAMPLE_FRACTION" != "None" ] && [ "$SUBSAMPLE_FRACTION" != "" ] && [ "$SUBSAMPLE_FRACTION" != "0" ]; then
            HAS_FRACTION=true
        else
            HAS_FRACTION=false
        fi

        # Check if max_reads is set
        if [ "$SUBSAMPLE_MAX_READS" != "None" ] && [ "$SUBSAMPLE_MAX_READS" != "" ]; then
            HAS_MAX_READS=true
        else
            HAS_MAX_READS=false
        fi

        # Determine mode and calculate final fraction
        if [ "$HAS_FRACTION" = true ] && [ "$HAS_MAX_READS" = true ]; then
            # Mode 3: Fraction with max reads cap
            echo "MODE 3: Fraction ($SUBSAMPLE_FRACTION) with max reads cap ($SUBSAMPLE_MAX_READS)" >> {log}
            FRACTION_READS=$(echo "scale=0; $ORIGINAL_READS * $SUBSAMPLE_FRACTION / 1" | bc -l)
            if [ $FRACTION_READS -gt $SUBSAMPLE_MAX_READS ]; then
                FINAL_READS=$SUBSAMPLE_MAX_READS
                ACTUAL_FRACTION=$(echo "scale=6; $FINAL_READS / $ORIGINAL_READS" | bc -l)
                echo "Fraction would give $FRACTION_READS reads, capped to $FINAL_READS reads" >> {log}
                echo "Using capped fraction: $ACTUAL_FRACTION" >> {log}
            else
                FINAL_READS=$FRACTION_READS
                ACTUAL_FRACTION=$SUBSAMPLE_FRACTION
                echo "Fraction gives $FINAL_READS reads, under max limit" >> {log}
                echo "Using requested fraction: $ACTUAL_FRACTION" >> {log}
            fi

        elif [ "$HAS_MAX_READS" = true ] && [ "$HAS_FRACTION" = false ]; then
            # Mode 2: Max reads only
            echo "MODE 2: Max reads only ($SUBSAMPLE_MAX_READS)" >> {log}
            if [ $ORIGINAL_READS -gt $SUBSAMPLE_MAX_READS ]; then
                ACTUAL_FRACTION=$(echo "scale=6; $SUBSAMPLE_MAX_READS / $ORIGINAL_READS" | bc -l)
                echo "Original reads ($ORIGINAL_READS) > max reads ($SUBSAMPLE_MAX_READS)" >> {log}
                echo "Using calculated fraction: $ACTUAL_FRACTION" >> {log}
            else
                ACTUAL_FRACTION=1.0
                echo "Original reads ($ORIGINAL_READS) <= max reads ($SUBSAMPLE_MAX_READS), keeping all reads" >> {log}
                echo "Using fraction: $ACTUAL_FRACTION" >> {log}
            fi

        elif [ "$HAS_FRACTION" = true ] && [ "$HAS_MAX_READS" = false ]; then
            # Mode 1: Fraction only
            echo "MODE 1: Fraction only ($SUBSAMPLE_FRACTION)" >> {log}
            ACTUAL_FRACTION=$SUBSAMPLE_FRACTION
            echo "Using requested fraction: $ACTUAL_FRACTION" >> {log}

        else
            # Error: neither set
            echo "ERROR: Neither SUBSAMPLE_FRACTION nor SUBSAMPLE_MAX_READS is set!" >> {log}
            echo "This should not happen if ENABLE_SUBSAMPLING is true." >> {log}
            exit 1
        fi

        # Subsample the BAM file - samtools -s expects seed.fraction format
        # We use a fixed seed (1234) and append the fraction
        SEED_FRACTION="1234$ACTUAL_FRACTION"
        echo "Samtools seed.fraction parameter: $SEED_FRACTION" >> {log}
        samtools view -s $SEED_FRACTION -b {input.bam} > {output.bam} 2>> {log}

        # Count subsampled reads
        SUBSAMPLED_READS=$(samtools view -c {output.bam})
        echo "Subsampled reads: $SUBSAMPLED_READS" >> {log}

        # Append aggregation summary to log for downstream parsing
        DATE=$(date +%Y-%m-%d)
        echo -e "SUMMARY\t{params.sample_id}\t$DATE\t$ORIGINAL_READS\t$SUBSAMPLED_READS\t$ACTUAL_FRACTION" >> {log}

        echo "Subsampling completed successfully" >> {log}
        """


rule aggregate_read_counts:
    """Aggregate read count statistics from all subsampled files into a single log file."""
    input:
        subsample_logs=(
            [
                f"logs/sr2silo/subsample/sampleId_{sample_id}.log"
                for sample_id in SAMPLE_IDS
            ]
            if config.get("ENABLE_SUBSAMPLING", False)
            else []
        ),
    output:
        summary=f"{config['LOG_DIR']}/subsampling/read_counts.tsv",
    run:
        from pathlib import Path

        summary_path = Path(output.summary)
        summary_path.parent.mkdir(parents=True, exist_ok=True)

        lines = [
            "sample_id\tdate\toriginal_reads\tsubsampled_reads\tsubsample_fraction"
        ]

        log_entries = input.subsample_logs
        if isinstance(log_entries, str):
            log_entries = [log_entries]

        for log_path in map(Path, log_entries):
            if not log_path.exists():
                continue

            summary_line = None
            with log_path.open() as handle:
                for line in handle:
                    if line.startswith("SUMMARY\t"):
                        # Keep the latest SUMMARY line emitted by the subsample job
                        summary_line = line.strip().split("\t")

            if summary_line:
                lines.append("\t".join(summary_line[1:]))
                continue

        summary_path.write_text("\n".join(lines) + "\n")
        print(f"Read count summary created: {output.summary}")


rule process_sample:
    """Processes the sample to ndjson, skip upload to loculus.

    This rule demonstrates the flexible parameter approach:
    - Parameters can be provided via CLI arguments (as shown)
    - Or via environment variables (TIMELINE_FILE, PRIMER_FILE, NEXTCLADE_REFERENCE)
    - CLI arguments take precedence over environment variables

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        sample_fp=lambda wildcards: get_processing_input_path(wildcards.sample_id),
    output:
        result_fp=f"{config['RESULTS_DIR']}" + "/sampleId-{sample_id}.ndjson.zst",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        timeline_file=config["TIMELINE_FILE"],
        organism=config.get("ORGANISM", "covid"),
        lapis_url=config.get("LAPIS_URL", ""),
    log:
        f"{config['LOG_DIR']}/sr2silo/process_sample/sampleId_{{sample_id}}.log",
    conda:
        "envs/sr2silo.yaml"
    retries: 3
    shell:
        """
        sr2silo process-from-vpipe \
            --input-file {input.sample_fp} \
            --sample-id {params.sample_id} \
            --timeline-file {params.timeline_file} \
            --organism {params.organism} \
            --output-fp {output.result_fp} \
            $([ -n "{params.lapis_url}" ] && echo "--lapis-url {params.lapis_url}") \
            > {log} 2>&1
        """


rule submit_to_loculus:
    """Submits the processed sample to loculus.

    This rule now uses CLI parameters with environment variable fallbacks.
    The sr2silo command will automatically use environment variables
    if CLI parameters are not provided.

    Required configuration (via config file or environment variables):
        - KEYCLOAK_TOKEN_URL
        - BACKEND_URL
        - GROUP_ID
        - USERNAME (environment variable, set by secrets.py)
        - PASSWORD (environment variable, set by secrets.py)
        - ORGANISM

    Optional configuration:
        - AUTO_RELEASE: If true, automatically release sequences after submission
        - RELEASE_DELAY: Seconds to wait before releasing (default: 180)

    CLI parameters take precedence over environment variables.

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        result_fp=f"{config['RESULTS_DIR']}" + "/sampleId-{sample_id}.ndjson.zst",
        nucleotide_alignment=f"{config['RESULTS_DIR']}/subsampled/{{sample_id}}/REF_aln_trim.bam",
    output:
        flag=f"{config['RESULTS_DIR']}/uploads/sampleId-{{sample_id}}.uploaded",
    params:
        keycloak_url=config.get("KEYCLOAK_TOKEN_URL", ""),
        backend_url=config.get("BACKEND_URL", ""),
        group_id=config.get("GROUP_ID", ""),
        # USERNAME and PASSWORD come from environment variables (set by secrets.py)
        username=os.environ.get("USERNAME", ""),
        password=os.environ.get("PASSWORD", ""),
        organism=config.get("ORGANISM", ""),
        auto_release="--auto-release" if config.get("AUTO_RELEASE", False) else "",
        release_delay=config.get("RELEASE_DELAY", 180),
    log:
        f"{config['LOG_DIR']}/sr2silo/submit_to_loculus/sampleId_{{sample_id}}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo submit-to-loculus \
            --processed-file {input.result_fp} \
            --nucleotide-alignment {input.nucleotide_alignment} \
            --keycloak-token-url "{params.keycloak_url}" \
            --backend-url "{params.backend_url}" \
            --group-id {params.group_id} \
            --organism "{params.organism}" \
            --username "{params.username}" \
            --password "{params.password}" \
            {params.auto_release} \
            --release-delay {params.release_delay} > {log} 2>&1 && \
        mkdir -p $(dirname {output.flag}) && \
        touch {output.flag}
        """
