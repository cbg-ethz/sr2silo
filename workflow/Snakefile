"""Workflow to process historical samples of V-Pipe nucleotide alignment to
    SILO ready ndjson.zst files.
"""

import yaml
import os
import sys
import csv
from pathlib import Path
from datetime import datetime

# Prefer Snakemake's notion of the workflow base directory, but fall back to this file's directory for direct execution.
try:
    WORKFLOW_DIR = Path(workflow.basedir)
except NameError:
    WORKFLOW_DIR = Path(__file__).resolve().parent

# Configuration and path resolution notes:
# - When running tests, the suite copies a workflow/config.yaml into a temporary workdir and
#   invokes Snakemake with `--directory <workdir>`. In that case we prefer the runtime
#   config at `<workdir>/workflow/config.yaml` so tests remain self-contained.
# - For file paths provided in config (e.g., TIMELINE_FILE, BASE_SAMPLE_DIR), we resolve
#   relative paths first against the runtime workdir, then the config directory, and finally
#   this repository's workflow directory. This keeps CLI runs and CI tests consistent.

# Also capture Snakemake's runtime workdir (when --directory is used), fall back to current CWD
try:
    RUNTIME_DIR = Path(workflow.workdir)
except Exception:
    RUNTIME_DIR = Path.cwd()

# Prefer a config from the runtime workdir (tests copy theirs into workdir/workflow/config.yaml),
# and fall back to the repository's workflow/config.yaml
DEFAULT_CONFIG = WORKFLOW_DIR / "config_prod.yaml" # for production runs
RUNTIME_CONFIG = RUNTIME_DIR / "workflow" / "config.yaml" # for CI runs
CONFIG = RUNTIME_CONFIG if RUNTIME_CONFIG.exists() else DEFAULT_CONFIG
if not CONFIG.exists():
    print(f"❌ ERROR: Expected config file not found: {CONFIG}")
    sys.exit(1)


configfile: str(CONFIG)


# Load configuration
with open(CONFIG, "r") as file:
    config = yaml.safe_load(file)

# Helper to resolve relative paths from workdir/config/workflow dirs
CONFIG_DIR = CONFIG.parent


def resolve_path(p):
    """Resolve a path string relative to (in order): runtime workdir, config dir, workflow dir.

    - Absolute paths are returned unchanged.
    - For relative paths, we return the first existing candidate; if none exist,
      we return the path relative to the config directory (so subsequent creation works).
    """
    p = Path(p)
    if p.is_absolute():
        return p
    for base in (RUNTIME_DIR, CONFIG_DIR, WORKFLOW_DIR):
        candidate = base / p
        if candidate.exists():
            return candidate
    return CONFIG_DIR / p


def resolve_date(date_str):
    """Resolve date string, supporting 'TODAY' and relative dates like '2_WEEKS_AGO', '1_MONTH_AGO'."""
    date_upper = date_str.upper()

    if date_upper == "TODAY":
        return datetime.now().strftime("%Y-%m-%d")

    # Handle relative dates: N_WEEKS_AGO, N_MONTHS_AGO, N_DAYS_AGO
    import re
    from datetime import timedelta
    from dateutil.relativedelta import relativedelta

    # Match patterns like "2_WEEKS_AGO", "1_MONTH_AGO", "30_DAYS_AGO"
    weeks_match = re.match(r"(\d+)_WEEKS?_AGO", date_upper)
    months_match = re.match(r"(\d+)_MONTHS?_AGO", date_upper)
    days_match = re.match(r"(\d+)_DAYS?_AGO", date_upper)

    now = datetime.now()

    if weeks_match:
        weeks = int(weeks_match.group(1))
        target_date = now - timedelta(weeks=weeks)
        return target_date.strftime("%Y-%m-%d")

    elif months_match:
        months = int(months_match.group(1))
        target_date = now - relativedelta(months=months)
        return target_date.strftime("%Y-%m-%d")

    elif days_match:
        days = int(days_match.group(1))
        target_date = now - timedelta(days=days)
        return target_date.strftime("%Y-%m-%d")

    # If no special pattern, return as-is (assume explicit date)
    return date_str


# Resolve dynamic dates in config
config["START_DATE"] = resolve_date(config["START_DATE"])
config["END_DATE"] = resolve_date(config["END_DATE"])


def get_sample_list():
    """Read timeline file and filter by date range and location codes."""

    # Read the timeline file
    sample_list = []

    timeline_fp = resolve_path(config["TIMELINE_FILE"])
    with open(timeline_fp, "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Skip header row if it exists
        first_row = next(reader, None)
        if first_row and first_row[0] == "sample":
            # This is a header row, continue with the next rows
            pass
        else:
            # This is data, process it
            if first_row and len(first_row) >= 7:
                sample, batch, reads, proto, location_code, date_str, location = (
                    first_row
                )

                # Convert date string to datetime for comparison
                try:
                    sample_date = datetime.strptime(date_str, "%Y-%m-%d")
                    location_code_int = int(location_code)

                    # Filter by date range
                    start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
                    end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

                    if (
                        start_date <= sample_date <= end_date
                        and location_code_int in config["LOCATIONS"]
                    ):
                        sample_list.append(sample)
                except (ValueError, TypeError):
                    logging.warning(f"Skipping invalid row: {first_row}")
                    pass  # Skip invalid rows

        # Process remaining rows
        for row in reader:
            if len(row) < 7:  # Skip invalid rows
                continue

            sample, batch, reads, proto, location_code, date_str, location = row

            # Convert date string to datetime for comparison
            try:
                sample_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                continue  # Skip rows with invalid date format

            # Convert location_code to int for comparison
            try:
                location_code_int = int(location_code)
            except ValueError:
                continue  # Skip rows with invalid location code

            # Filter by date range
            start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
            end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

            if not (start_date <= sample_date <= end_date):
                continue

            # Filter by location codes
            if location_code_int not in config["LOCATIONS"]:
                continue

            # Add to sample list (avoid duplicates)
            if sample not in sample_list:
                sample_list.append(sample)

    return sample_list


def get_input_file_path(sample_id):
    """Get the input file path for the sample, trying both with and without batch structure."""
    base_dir = resolve_path(config["BASE_SAMPLE_DIR"])

    # First try without batch_id structure
    simple_path = base_dir / sample_id / "alignments" / "REF_aln_trim.bam"
    if simple_path.exists():
        return str(simple_path)

    # If that doesn't exist, we need to find the batch_id by checking the timeline
    timeline_fp = resolve_path(config["TIMELINE_FILE"])
    with open(timeline_fp, "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Skip header row if it exists
        first_row = next(reader, None)
        if first_row and first_row[0] == "sample":
            pass  # Header row
        else:
            # Check first row if it's not a header
            if first_row and len(first_row) >= 2 and first_row[0] == sample_id:
                batch_id = first_row[1]
                if batch_id:  # If batch_id exists, try that path
                    batch_path = (
                        base_dir
                        / sample_id
                        / batch_id
                        / "alignments"
                        / "REF_aln_trim.bam"
                    )
                    if batch_path.exists():
                        return str(batch_path)

        # Process remaining rows
        for row in reader:
            if len(row) >= 2 and row[0] == sample_id:
                batch_id = row[1]
                if batch_id:  # If batch_id exists, try that path
                    batch_path = (
                        base_dir
                        / sample_id
                        / batch_id
                        / "alignments"
                        / "REF_aln_trim.bam"
                    )
                    if batch_path.exists():
                        return str(batch_path)

    # Default to simple path if nothing else works
    return str(simple_path)


def get_processing_input_path(sample_id):
    """Get the appropriate input path for processing - subsampled if enabled, original otherwise."""
    if config.get("ENABLE_SUBSAMPLING", False):
        return f"{config['RESULTS_DIR']}/subsampled/{sample_id}/REF_aln_trim.bam"
    else:
        return get_input_file_path(sample_id)


# Get the filtered sample list
SAMPLE_IDS = get_sample_list()

# Validate subsampling configuration
if config.get("ENABLE_SUBSAMPLING", False):
    fraction = config.get("SUBSAMPLE_FRACTION")
    max_reads = config.get("SUBSAMPLE_MAX_READS")

    # Check if at least one subsampling parameter is set
    fraction_set = fraction is not None and fraction != "" and fraction != 0
    max_reads_set = max_reads is not None and max_reads != ""

    if not fraction_set and not max_reads_set:
        print(
            "❌ ERROR: ENABLE_SUBSAMPLING is true but neither SUBSAMPLE_FRACTION nor SUBSAMPLE_MAX_READS is set!"
        )
        print("   Please set at least one of:")
        print("   - SUBSAMPLE_FRACTION (e.g., 0.1 for 10%)")
        print("   - SUBSAMPLE_MAX_READS (e.g., 1000 for max 1000 reads)")
        print("   Or set ENABLE_SUBSAMPLING: false to disable subsampling")
        sys.exit(1)

    # Log the subsampling mode that will be used
    if fraction_set and max_reads_set:
        print(
            f"🎯 Subsampling Mode 3: {fraction*100:.1f}% of reads, capped at {max_reads:,} reads"
        )
    elif max_reads_set:
        print(f"🎯 Subsampling Mode 2: Maximum {max_reads:,} reads")
    elif fraction_set:
        print(f"🎯 Subsampling Mode 1: {fraction*100:.1f}% of reads")

# Warning if no samples found
if not SAMPLE_IDS:
    print("⚠️  WARNING: No samples found matching the filtering criteria!")
    print(f"   Date range: {config['START_DATE']} to {config['END_DATE']}")
    print(f"   Location codes: {config['LOCATIONS']}")
    print(f"   Timeline file: {config['TIMELINE_FILE']}")
    print("   Please check your configuration and timeline file.")
    print("   The workflow will complete with no jobs to execute.")
else:
    print(f"✓ Found {len(SAMPLE_IDS)} samples matching criteria:")
    for sample_id in SAMPLE_IDS:
        print(f"   {sample_id}")
    print()


rule all:
    input:
        [
            f"{config['RESULTS_DIR']}/sampleId-{sample_id}.ndjson.zst"
            for sample_id in SAMPLE_IDS
        ],
        [
            f"{config['RESULTS_DIR']}/uploads/sampleId-{sample_id}.uploaded"
            for sample_id in SAMPLE_IDS
        ],
        # Add subsampled BAM files if subsampling is enabled
        [
            f"{config['RESULTS_DIR']}/subsampled/{sample_id}/REF_aln_trim.bam"
            for sample_id in SAMPLE_IDS
        ]
        if config.get("ENABLE_SUBSAMPLING", False)
        else [],
        # Add read count summary if subsampling is enabled
        ["logs/subsampling/read_counts.tsv"]
        if config.get("ENABLE_SUBSAMPLING", False)
        else [],


rule subsample_bam:
    """Subsample BAM files to reduce computational load and standardize input sizes.

    Tracks original and subsampled read counts for downstream analysis.
    Uses samtools view with -s flag for random subsampling.
    """
    input:
        bam=lambda wildcards: get_input_file_path(wildcards.sample_id),
    output:
        bam=f"{config['RESULTS_DIR']}/subsampled/{{sample_id}}/REF_aln_trim.bam",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        subsample_fraction=config.get("SUBSAMPLE_FRACTION", "None"),
        subsample_max_reads=config.get("SUBSAMPLE_MAX_READS", "None"),
    log:
        "logs/sr2silo/subsample/sampleId_{sample_id}.log",
    conda:
        "envs/subsampling.yaml"
    shell:
        """
        # Create output directory
        mkdir -p $(dirname {output.bam})
        mkdir -p logs/subsampling

        # Count original reads
        ORIGINAL_READS=$(samtools view -c {input.bam})
        echo "Original reads: $ORIGINAL_READS" > {log}

        # Determine subsampling mode and calculate final fraction
        SUBSAMPLE_FRACTION="{params.subsample_fraction}"
        SUBSAMPLE_MAX_READS="{params.subsample_max_reads}"

        # Check if fraction is set (not None, not empty, not "None")
        if [ "$SUBSAMPLE_FRACTION" != "None" ] && [ "$SUBSAMPLE_FRACTION" != "" ] && [ "$SUBSAMPLE_FRACTION" != "0" ]; then
            HAS_FRACTION=true
        else
            HAS_FRACTION=false
        fi

        # Check if max_reads is set
        if [ "$SUBSAMPLE_MAX_READS" != "None" ] && [ "$SUBSAMPLE_MAX_READS" != "" ]; then
            HAS_MAX_READS=true
        else
            HAS_MAX_READS=false
        fi

        # Determine mode and calculate final fraction
        if [ "$HAS_FRACTION" = true ] && [ "$HAS_MAX_READS" = true ]; then
            # Mode 3: Fraction with max reads cap
            echo "MODE 3: Fraction ($SUBSAMPLE_FRACTION) with max reads cap ($SUBSAMPLE_MAX_READS)" >> {log}
            FRACTION_READS=$(echo "scale=0; $ORIGINAL_READS * $SUBSAMPLE_FRACTION / 1" | bc -l)
            if [ $FRACTION_READS -gt $SUBSAMPLE_MAX_READS ]; then
                FINAL_READS=$SUBSAMPLE_MAX_READS
                ACTUAL_FRACTION=$(echo "scale=6; $FINAL_READS / $ORIGINAL_READS" | bc -l)
                echo "Fraction would give $FRACTION_READS reads, capped to $FINAL_READS reads" >> {log}
                echo "Using capped fraction: $ACTUAL_FRACTION" >> {log}
            else
                FINAL_READS=$FRACTION_READS
                ACTUAL_FRACTION=$SUBSAMPLE_FRACTION
                echo "Fraction gives $FINAL_READS reads, under max limit" >> {log}
                echo "Using requested fraction: $ACTUAL_FRACTION" >> {log}
            fi

        elif [ "$HAS_MAX_READS" = true ] && [ "$HAS_FRACTION" = false ]; then
            # Mode 2: Max reads only
            echo "MODE 2: Max reads only ($SUBSAMPLE_MAX_READS)" >> {log}
            if [ $ORIGINAL_READS -gt $SUBSAMPLE_MAX_READS ]; then
                ACTUAL_FRACTION=$(echo "scale=6; $SUBSAMPLE_MAX_READS / $ORIGINAL_READS" | bc -l)
                echo "Original reads ($ORIGINAL_READS) > max reads ($SUBSAMPLE_MAX_READS)" >> {log}
                echo "Using calculated fraction: $ACTUAL_FRACTION" >> {log}
            else
                ACTUAL_FRACTION=1.0
                echo "Original reads ($ORIGINAL_READS) <= max reads ($SUBSAMPLE_MAX_READS), keeping all reads" >> {log}
                echo "Using fraction: $ACTUAL_FRACTION" >> {log}
            fi

        elif [ "$HAS_FRACTION" = true ] && [ "$HAS_MAX_READS" = false ]; then
            # Mode 1: Fraction only
            echo "MODE 1: Fraction only ($SUBSAMPLE_FRACTION)" >> {log}
            ACTUAL_FRACTION=$SUBSAMPLE_FRACTION
            echo "Using requested fraction: $ACTUAL_FRACTION" >> {log}

        else
            # Error: neither set
            echo "ERROR: Neither SUBSAMPLE_FRACTION nor SUBSAMPLE_MAX_READS is set!" >> {log}
            echo "This should not happen if ENABLE_SUBSAMPLING is true." >> {log}
            exit 1
        fi

        # Subsample the BAM file - samtools -s expects seed.fraction format
        # We use a fixed seed (1234) and append the fraction
        SEED_FRACTION="1234$ACTUAL_FRACTION"
        echo "Samtools seed.fraction parameter: $SEED_FRACTION" >> {log}
        samtools view -s $SEED_FRACTION -b {input.bam} > {output.bam} 2>> {log}

        # Count subsampled reads
        SUBSAMPLED_READS=$(samtools view -c {output.bam})
        echo "Subsampled reads: $SUBSAMPLED_READS" >> {log}

        # Append aggregation summary to log for downstream parsing
        DATE=$(date +%Y-%m-%d)
        echo -e "SUMMARY\t{params.sample_id}\t$DATE\t$ORIGINAL_READS\t$SUBSAMPLED_READS\t$ACTUAL_FRACTION" >> {log}

        echo "Subsampling completed successfully" >> {log}
        """


rule aggregate_read_counts:
    """Aggregate read count statistics from all subsampled files into a single log file."""
    input:
        subsample_logs=(
            [
                f"logs/sr2silo/subsample/sampleId_{sample_id}.log"
                for sample_id in SAMPLE_IDS
            ]
            if config.get("ENABLE_SUBSAMPLING", False)
            else []
        ),
    output:
        summary="logs/subsampling/read_counts.tsv",
    run:
        from pathlib import Path

        summary_path = Path(output.summary)
        summary_path.parent.mkdir(parents=True, exist_ok=True)

        lines = [
            "sample_id\tdate\toriginal_reads\tsubsampled_reads\tsubsample_fraction"
        ]

        log_entries = input.subsample_logs
        if isinstance(log_entries, str):
            log_entries = [log_entries]

        for log_path in map(Path, log_entries):
            if not log_path.exists():
                continue

            summary_line = None
            with log_path.open() as handle:
                for line in handle:
                    if line.startswith("SUMMARY\t"):
                        # Keep the latest SUMMARY line emitted by the subsample job
                        summary_line = line.strip().split("\t")

            if summary_line:
                lines.append("\t".join(summary_line[1:]))
                continue

        summary_path.write_text("\n".join(lines) + "\n")
        print(f"Read count summary created: {output.summary}")


rule process_sample:
    """Processes the sample to ndjson, skip upload to loculus.

    This rule demonstrates the flexible parameter approach:
    - Parameters can be provided via CLI arguments (as shown)
    - Or via environment variables (TIMELINE_FILE, PRIMER_FILE, NEXTCLADE_REFERENCE)
    - CLI arguments take precedence over environment variables

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        sample_fp=lambda wildcards: get_processing_input_path(wildcards.sample_id),
    output:
        result_fp=f"{config['RESULTS_DIR']}" + "/sampleId-{sample_id}.ndjson.zst",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        timeline_file=config["TIMELINE_FILE"],
        lapis_url=config.get("LAPIS_URL", ""),
    log:
        "logs/sr2silo/process_sample/sampleId_{sample_id}.log",
    conda:
        "envs/sr2silo.yaml"
    retries: 3
    shell:
        """
        sr2silo process-from-vpipe \
            --input-file {input.sample_fp} \
            --sample-id {params.sample_id} \
            --timeline-file {params.timeline_file} \
            --output-fp {output.result_fp} \
            --lapis-url {params.lapis_url} > {log} 2>&1
        """


rule submit_to_loculus:
    """Submits the processed sample to loculus.

    This rule now uses CLI parameters with environment variable fallbacks.
    The sr2silo command will automatically use environment variables
    if CLI parameters are not provided.

    Required configuration (via config file or environment variables):
        - KEYCLOAK_TOKEN_URL
        - BACKEND_URL
        - GROUP_ID
        - USERNAME
        - PASSWORD
        - ORGANISM

    CLI parameters take precedence over environment variables.

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        result_fp=f"{config['RESULTS_DIR']}" + "/sampleId-{sample_id}.ndjson.zst",
        nucleotide_alignment=f"{config['RESULTS_DIR']}/subsampled/{{sample_id}}/REF_aln_trim.bam",
    output:
        flag=f"{config['RESULTS_DIR']}/uploads/sampleId-{{sample_id}}.uploaded",
    params:
        keycloak_url=config.get("KEYCLOAK_TOKEN_URL", ""),
        backend_url=config.get("BACKEND_URL", ""),
        group_id=config.get("GROUP_ID", ""),
        username=config.get("USERNAME", ""),
        password=config.get("PASSWORD", ""),
        organism=config.get("ORGANISM", ""),
    log:
        "logs/sr2silo/submit_to_loculus/sampleId_{sample_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo submit-to-loculus \
            --processed-file {input.result_fp} \
            --nucleotide-alignment {input.nucleotide_alignment} \
            --keycloak-token-url "{params.keycloak_url}" \
            --backend-url "{params.backend_url}" \
            --group-id {params.group_id} \
            --organism "{params.organism}" \
            --username {params.username} \
            --password {params.password} > {log} 2>&1 && \
        mkdir -p $(dirname {output.flag}) && \
        touch {output.flag}
        """
