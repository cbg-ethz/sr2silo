"""Workflow to process historical samples of V-Pipe nucleotide alignment to
    SILO ready ndjson.zst files.
"""

import yaml
import os
from pathlib import Path


CONFIG = "workflow/config.yaml"
# CONFIG = "workflow/config_euler.yaml"


configfile: CONFIG


# Load configuration
with open(CONFIG, "r") as file:
    config = yaml.safe_load(file)


def get_batch_id(sample_id):
    """Get the batch ids from the sample batch ids."""
    return config["SAMPLE_BATCH_IDS"][sample_id]


rule all:
    input:
        [
            f"{config['RESULTS_DIR']}/sampleId-{sample_id}_batchId-{get_batch_id(sample_id)}.ndjson.zst"
            for sample_id in config["SAMPLE_BATCH_IDS"].keys()
        ],


rule process_sample:
    """Processes the sample to ndjson, skip upload to loculus."""
    input:
        sample_fp=f"{config['BASE_SAMPLE_DIR']}"
        + "/{sample_id}/{batch_id}/alignments/REF_aln_trim.bam",
    output:
        result_fp=f"{config['RESULTS_DIR']}"
        + "/sampleId-{sample_id}_batchId-{batch_id}.ndjson.zst",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        batch_id=lambda wildcards: wildcards.batch_id,
        timeline_file=config["TIMELINE_FILE"],
        primers_file=config["PRIMERS_FILE"],
        nuc_reference=config["NUC_REFERENCE"],
        aa_reference=config["NUC_REFERENCE"],
    log:
        "logs/sr2silo/process_sample/sampleId_{sample_id}_batchId_{batch_id}.log",  # Clearer wildcard separation
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo process-from-vpipe \
            --input-file {input.sample_fp} \
            --sample-id {params.sample_id} \
            --batch-id {params.batch_id} \
            --timeline-file {params.timeline_file} \
            --primer-file {params.primers_file} \
            --output-fp {output.result_fp} \
            --reference {params.nuc_reference} > {log} 2>&1
        """


rule submit_to_loculus:
    """Submits the processed sample to loculus.

    Note the required environment variables:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_DEFAULT_REGION
        - KEYCLOAK_TOKEN_URL
        - SUBMISSION_URL
    Configured in the workflow/config.yaml file.

    Development Note:
    With Loculus File Sharing and S3 managed uploads, this rule
    will get simpler in the future, abstracting s3 locations away.
    """
    input:
        result_fp=f"{config['RESULTS_DIR']}"
        + "/sampleId-{sample_id}_batchId-{batch_id}.ndjson.zst",
    output:
        flag=f"{config['RESULTS_DIR']}/uploads/sampleId-{{sample_id}}_batchId-{{batch_id}}.uploaded",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
    log:
        "logs/sr2silo/submit_to_loculus/sampleId_{sample_id}_batchId_{batch_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        (
            """
        export AWS_ACCESS_KEY_ID="""
            + config.get("AWS_ACCESS_KEY_ID", "")
            + """; \
                                        export AWS_SECRET_ACCESS_KEY="""
            + config.get("AWS_SECRET_ACCESS_KEY", "")
            + """; \
                                        export AWS_DEFAULT_REGION="""
            + config.get("AWS_DEFAULT_REGION", "us-east-1")
            + """; \
                                        export KEYCLOAK_TOKEN_URL="""
            + config.get("KEYCLOAK_TOKEN_URL", "")
            + """; \
                                        export SUBMISSION_URL="""
            + config.get("SUBMISSION_URL", "")
            + """; \
                                        sr2silo submit-to-loculus \
                                            --processed-file {input.result_fp} \
                                            --sample-id {params.sample_id} > {log} 2>&1 && \
                                        mkdir -p $(dirname {output.flag}) && \
                                        touch {output.flag}
                                        """
        )
