"""Workflow to process historical samples of V-Pipe nucleotide alignment to
    SILO ready ndjson.zst files.
"""

import yaml
import os
import csv
from pathlib import Path
from datetime import datetime


CONFIG = "workflow/config.yaml"
# CONFIG = "workflow/config_euler.yaml"


configfile: CONFIG


# Load configuration
with open(CONFIG, "r") as file:
    config = yaml.safe_load(file)


def get_sample_batch_mapping():
    """Read timeline file and filter by date range and location codes."""

    # Read the timeline file
    sample_batch_mapping = {}

    with open(config["TIMELINE_FILE"], "r") as f:
        reader = csv.reader(f, delimiter="\t")
        # Skip header row if it exists
        first_row = next(reader, None)
        if first_row and first_row[0] == "sample":
            # This is a header row, continue with the next rows
            pass
        else:
            # This is data, process it
            if first_row and len(first_row) >= 7:
                sample, batch, reads, proto, location_code, date_str, location = (
                    first_row
                )

                # Convert date string to datetime for comparison
                try:
                    sample_date = datetime.strptime(date_str, "%Y-%m-%d")
                    location_code_int = int(location_code)

                    # Filter by date range
                    start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
                    end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

                    if (
                        start_date <= sample_date <= end_date
                        and location_code_int in config["LOCATIONS"]
                    ):
                        sample_batch_mapping[sample] = batch
                except (ValueError, TypeError):
                    pass  # Skip invalid rows

        # Process remaining rows
        for row in reader:
            if len(row) < 7:  # Skip invalid rows
                continue

            sample, batch, reads, proto, location_code, date_str, location = row

            # Convert date string to datetime for comparison
            try:
                sample_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                continue  # Skip rows with invalid date format

            # Convert location_code to int for comparison
            try:
                location_code_int = int(location_code)
            except ValueError:
                continue  # Skip rows with invalid location code

            # Filter by date range
            start_date = datetime.strptime(config["START_DATE"], "%Y-%m-%d")
            end_date = datetime.strptime(config["END_DATE"], "%Y-%m-%d")

            if not (start_date <= sample_date <= end_date):
                continue

            # Filter by location codes
            if location_code_int not in config["LOCATIONS"]:
                continue

            # Add to mapping
            sample_batch_mapping[sample] = batch

    return sample_batch_mapping


def get_batch_id(sample_id):
    """Get the batch id from the filtered sample batch mapping."""
    sample_batch_mapping = get_sample_batch_mapping()
    return sample_batch_mapping[sample_id]


# Get the filtered sample-batch mapping
SAMPLE_BATCH_IDS = get_sample_batch_mapping()

# Warning if no samples found
if not SAMPLE_BATCH_IDS:
    print("⚠️  WARNING: No samples found matching the filtering criteria!")
    print(f"   Date range: {config['START_DATE']} to {config['END_DATE']}")
    print(f"   Location codes: {config['LOCATIONS']}")
    print(f"   Timeline file: {config['TIMELINE_FILE']}")
    print("   Please check your configuration and timeline file.")
    print("   The workflow will complete with no jobs to execute.")
else:
    print(f"✓ Found {len(SAMPLE_BATCH_IDS)} samples matching criteria:")
    for sample_id, batch_id in SAMPLE_BATCH_IDS.items():
        print(f"   {sample_id} -> {batch_id}")
    print()


rule all:
    input:
        [
            f"{config['RESULTS_DIR']}/sampleId-{sample_id}_batchId-{get_batch_id(sample_id)}.ndjson.zst"
            for sample_id in SAMPLE_BATCH_IDS.keys()
        ],
        [
            f"{config['RESULTS_DIR']}/uploads/sampleId-{sample_id}_batchId-{get_batch_id(sample_id)}.uploaded"
            for sample_id in SAMPLE_BATCH_IDS.keys()
        ],


rule process_sample:
    """Processes the sample to ndjson, skip upload to loculus.

    This rule demonstrates the flexible parameter approach:
    - Parameters can be provided via CLI arguments (as shown)
    - Or via environment variables (TIMELINE_FILE, PRIMER_FILE, NEXTCLADE_REFERENCE)
    - CLI arguments take precedence over environment variables

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        sample_fp=f"{config['BASE_SAMPLE_DIR']}"
        + "/{sample_id}/{batch_id}/alignments/REF_aln_trim.bam",
    output:
        result_fp=f"{config['RESULTS_DIR']}"
        + "/sampleId-{sample_id}_batchId-{batch_id}.ndjson.zst",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        batch_id=lambda wildcards: wildcards.batch_id,
        timeline_file=config["TIMELINE_FILE"],
        primers_file=config["PRIMERS_FILE"],
        nuc_reference=config["NUC_REFERENCE"],
        aa_reference=config["NUC_REFERENCE"],
    log:
        "logs/sr2silo/process_sample/sampleId_{sample_id}_batchId_{batch_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo process-from-vpipe \
            --input-file {input.sample_fp} \
            --sample-id {params.sample_id} \
            --batch-id {params.batch_id} \
            --timeline-file {params.timeline_file} \
            --primer-file {params.primers_file} \
            --output-fp {output.result_fp} \
            --reference {params.nuc_reference} > {log} 2>&1
        """


rule submit_to_loculus:
    """Submits the processed sample to loculus.

    This rule now uses CLI parameters with environment variable fallbacks.
    The sr2silo command will automatically use environment variables
    if CLI parameters are not provided.

    Required configuration (via config file or environment variables):
        - KEYCLOAK_TOKEN_URL
        - SUBMISSION_URL

    CLI parameters take precedence over environment variables.

    Note: On ETH cluster, internet access is provided by eth_proxy module loaded at job level.
    """
    input:
        result_fp=f"{config['RESULTS_DIR']}"
        + "/sampleId-{sample_id}_batchId-{batch_id}.ndjson.zst",
    output:
        flag=f"{config['RESULTS_DIR']}/uploads/sampleId-{{sample_id}}_batchId-{{batch_id}}.uploaded",
    params:
        sample_id=lambda wildcards: wildcards.sample_id,
        keycloak_url=config.get("KEYCLOAK_TOKEN_URL", ""),
        submission_url=config.get("SUBMISSION_URL", ""),
    log:
        "logs/sr2silo/submit_to_loculus/sampleId_{sample_id}_batchId_{batch_id}.log",
    conda:
        "envs/sr2silo.yaml"
    shell:
        """
        sr2silo submit-to-loculus \
            --processed-file {input.result_fp} \
            --sample-id {params.sample_id} \
            --keycloak-token-url "{params.keycloak_url}" \
            --submission-url "{params.submission_url}" > {log} 2>&1 && \
        mkdir -p $(dirname {output.flag}) && \
        touch {output.flag}
        """
